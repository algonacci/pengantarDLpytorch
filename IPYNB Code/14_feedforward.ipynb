{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feed Forward Neural Network\n",
    "Pada percobaan kali ini kita akan menggunakan dataset MNIST yaitu dataset yang berisi gambar-gambar angka tulisan tangan.\n",
    "\n",
    "**Import Library yang Digunakan**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Menggunakan GPU apabila memungkinkan**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hyperparameter**\n",
    "- input sebesar 794 didapat dari ukuran piksel pada satu buah gambar\n",
    "- output sebesar 10 didapat dari jumlah kelas yang ada, yaitu angka digit dari 0 - 9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mempersiapkan Dataset\n",
    "**Train Test**\n",
    "Torchvision menyediakan beberapa datasets yang kerap digunakan pada library ```torchvision.datasets```. Informasi lebih lanjut dapat dilihat pada [dokumentasi ```torchvision.datasets```](https://pytorch.org/vision/stable/datasets.html). Khusus untuk dataset MNIST, detail lebih lengkap dapat dilihat pada [tautan berikut ini](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST).\n",
    "- ```root='./data'```: Lokasi dataset yang akan digunakan.\n",
    "- ```train=True```: Menentukan dataset yang digunakan untuk training atau tidak.\n",
    "- ```transform=transforms.ToTensor()```: Transformasi yang akan dilakukan pada dataset.\n",
    "- ```download=True```: Download dataset jika belum ada.\n",
    "\n",
    "**DataLoader**\n",
    "Detail tentang DataLoader dapat dilihat pada [dokumentasi berikut ini](https://pytorch.org/docs/stable/data.html).\n",
    "- ```dataset```: Dataset yang akan digunakan.\n",
    "- ```batch_size```: Banyaknya data yang akan dibatasi dalam satu batch.\n",
    "- ```shuffle```: Untuk membuat data diacak atau tidak."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                           train=False,\n",
    "                                           transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran sample: torch.Size([100, 1, 28, 28])\n",
      "Ukuran label: torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x288 with 6 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_batch = iter(train_loader)\n",
    "samples, labels = example_batch.next()\n",
    "print(f'Ukuran sample: {samples.shape}')\n",
    "print(f'Ukuran label: {labels.shape}')\n",
    "\n",
    "# show 6 samples image with label using subplot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(samples[i][0], cmap='gist_yarg')\n",
    "    plt.title(\"Digit: {}\".format(labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Penjelasan Contoh Data**\n",
    "Terdapat 100 sampel data pada contoh batch yang dicetak dimana setiap gambar berukuran 28x28 pixel dan ditransformasi menjadi tensor dengan dimensi (1, 28, 28)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definisi Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loss dan Optimizer**\n",
    "- Kita akan menggunakan ```CrossEntropyLoss``` untuk menghitung loss.\n",
    "- Pada loss function itu sudah terdapat ```Softmax``` yang akan menghasilkan probabilitas untuk setiap kelas.\n",
    "- Untuk optimizer, kita akan menggunakan ```Adam``` optimizer.\n",
    "- ```optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training Loop**\n",
    "- images perlu di reshape karena input yang kita gunakan pada neural network berukuran 784\n",
    "- sebelumnya images memiliki dimensi (1, 28, 28) -> 1 channel, 28 pixel panjang, 28 pixel lebar\n",
    "- Diubah menjadi array dengan dimensi (1, 784)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.0019\n",
      "Epoch [1/2], Step [200/600], Loss: 0.0027\n",
      "Epoch [1/2], Step [300/600], Loss: 0.0079\n",
      "Epoch [1/2], Step [400/600], Loss: 0.0018\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0051\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0260\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0004\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0023\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0004\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0089\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0021\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "n_total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 97.07\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * n_correct / n_samples\n",
    "    print(f'Akurasi: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}